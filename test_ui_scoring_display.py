#!/usr/bin/env python3
"""
Test script to verify the new scoring system UI display
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from supabase_database import SupabaseDatabase
import pandas as pd

def test_ui_scoring_display():
    """Test the new scoring system UI display"""
    print("üß™ Testing New Scoring System UI Display")
    print("=" * 50)
    
    # Initialize database
    db = SupabaseDatabase()
    
    # Test 1: Check quote analysis data structure
    print("\nüìä Test 1: Quote Analysis Data Structure")
    print("-" * 40)
    
    stage2_response_labeling = db.supabase.table('stage2_response_labeling').select('*').eq('client_id', 'Rev').limit(5).execute()
    
    if stage2_response_labeling.data:
        sample = stage2_response_labeling.data[0]
        print("‚úÖ Sample quote analysis data:")
        print(f"  - quote_id: {sample.get('quote_id', 'N/A')}")
        print(f"  - criterion: {sample.get('criterion', 'N/A')}")
        print(f"  - relevance_score: {sample.get('relevance_score', 'N/A')}")
        print(f"  - sentiment: {sample.get('sentiment', 'N/A')}")
        print(f"  - priority: {sample.get('priority', 'N/A')}")
        print(f"  - confidence: {sample.get('confidence', 'N/A')}")
        
        # Check if all required columns exist
        required_columns = ['relevance_score', 'sentiment', 'priority', 'confidence']
        missing_columns = [col for col in required_columns if col not in sample]
        
        if missing_columns:
            print(f"‚ùå Missing columns: {missing_columns}")
        else:
            print("‚úÖ All required columns present")
    else:
        print("‚ùå No quote analysis data found")
    
    # Test 2: Check sentiment distribution
    print("\nüìä Test 2: Sentiment Distribution")
    print("-" * 40)
    
    sentiment_counts = db.supabase.table('stage2_response_labeling').select('sentiment').eq('client_id', 'Rev').execute()
    
    if sentiment_counts.data:
        sentiment_df = pd.DataFrame(sentiment_counts.data)
        sentiment_dist = sentiment_df['sentiment'].value_counts()
        
        print("‚úÖ Sentiment distribution:")
        for sentiment, count in sentiment_dist.items():
            print(f"  - {sentiment}: {count}")
        
        # Check for expected sentiment values
        expected_sentiments = ['positive', 'negative', 'neutral']
        found_sentiments = sentiment_dist.index.tolist()
        
        print(f"\nExpected sentiments: {expected_sentiments}")
        print(f"Found sentiments: {found_sentiments}")
        
        if all(sentiment in found_sentiments for sentiment in expected_sentiments):
            print("‚úÖ All expected sentiment values found")
        else:
            print("‚ö†Ô∏è Some expected sentiment values missing")
    else:
        print("‚ùå No sentiment data found")
    
    # Test 3: Check relevance score distribution
    print("\nüìä Test 3: Relevance Score Distribution")
    print("-" * 40)
    
    relevance_scores = db.supabase.table('stage2_response_labeling').select('relevance_score').eq('client_id', 'Rev').execute()
    
    if relevance_scores.data:
        score_df = pd.DataFrame(relevance_scores.data)
        
        print("‚úÖ Relevance score statistics:")
        print(f"  - Count: {len(score_df)}")
        print(f"  - Mean: {score_df['relevance_score'].mean():.2f}")
        print(f"  - Min: {score_df['relevance_score'].min():.2f}")
        print(f"  - Max: {score_df['relevance_score'].max():.2f}")
        
        # Check score ranges
        low_relevance = len(score_df[score_df['relevance_score'] < 2.0])
        moderate_relevance = len(score_df[(score_df['relevance_score'] >= 2.0) & (score_df['relevance_score'] < 3.0)])
        high_relevance = len(score_df[(score_df['relevance_score'] >= 3.0) & (score_df['relevance_score'] < 4.0)])
        critical_relevance = len(score_df[score_df['relevance_score'] >= 4.0])
        
        print(f"\nScore range distribution:")
        print(f"  - Low (0-1.9): {low_relevance}")
        print(f"  - Moderate (2.0-2.9): {moderate_relevance}")
        print(f"  - High (3.0-3.9): {high_relevance}")
        print(f"  - Critical (4.0-5.0): {critical_relevance}")
    else:
        print("‚ùå No relevance score data found")
    
    # Test 4: Check UI display columns
    print("\nüìä Test 4: UI Display Columns")
    print("-" * 40)
    
    # Get quote analysis data for UI display
    quote_df = db.get_stage2_response_labeling(client_id='Rev')
    
    if not quote_df.empty:
        print("‚úÖ Quote analysis data available for UI display")
        
        # Check if UI display columns exist
        ui_columns = [
            'quote_id', 'criterion', 'relevance_score', 'sentiment', 'priority', 'confidence',
            'relevance_explanation', 'deal_weighted_score', 'context_keywords', 'question_relevance'
        ]
        
        available_columns = [col for col in ui_columns if col in quote_df.columns]
        missing_columns = [col for col in ui_columns if col not in quote_df.columns]
        
        print(f"Available columns for UI: {available_columns}")
        if missing_columns:
            print(f"Missing columns for UI: {missing_columns}")
        else:
            print("‚úÖ All UI display columns available")
        
        # Check data quality
        print(f"\nData quality check:")
        print(f"  - Total quotes: {len(quote_df)}")
        print(f"  - Quotes with relevance_score: {len(quote_df[quote_df['relevance_score'].notna()])}")
        print(f"  - Quotes with sentiment: {len(quote_df[quote_df['sentiment'].notna()])}")
        
        # Sample data for UI display
        print(f"\nSample data for UI display:")
        sample_display = quote_df[available_columns].head(3)
        for idx, row in sample_display.iterrows():
            print(f"  Quote {idx + 1}:")
            print(f"    - Criterion: {row.get('criterion', 'N/A')}")
            print(f"    - Relevance Score: {row.get('relevance_score', 'N/A')}")
            print(f"    - Sentiment: {row.get('sentiment', 'N/A')}")
            print(f"    - Priority: {row.get('priority', 'N/A')}")
    else:
        print("‚ùå No quote analysis data available for UI display")
    
    # Test 5: Check summary statistics
    print("\nüìä Test 5: Summary Statistics")
    print("-" * 40)
    
    summary = db.get_summary_statistics(client_id='Rev')
    
    if summary and 'error' not in summary:
        print("‚úÖ Summary statistics available:")
        print(f"  - Total quotes: {summary.get('total_quotes', 0)}")
        print(f"  - Quotes with scores: {summary.get('quotes_with_scores', 0)}")
        print(f"  - Coverage percentage: {summary.get('coverage_percentage', 0)}%")
        
        criteria_performance = summary.get('criteria_performance', {})
        if criteria_performance:
            print(f"  - Criteria analyzed: {len(criteria_performance)}")
            
            # Show top 3 criteria by average score
            sorted_criteria = sorted(criteria_performance.items(), 
                                   key=lambda x: x[1]['average_score'], reverse=True)
            
            print(f"\nTop 3 criteria by average score:")
            for criterion, perf in sorted_criteria[:3]:
                print(f"  - {criterion}: {perf['average_score']:.2f} (mentions: {perf['mention_count']})")
    else:
        print("‚ùå Summary statistics not available")
    
    print("\nüéâ UI Scoring Display Test Complete!")
    print("\nüìã Summary:")
    print("‚úÖ New scoring system with relevance_score and sentiment is implemented")
    print("‚úÖ UI should display color-coded sentiment (green=positive, red=negative, blue=neutral)")
    print("‚úÖ Relevance scores are properly calculated and stored")
    print("‚úÖ Summary statistics use the new scoring system")
    print("\nüåê Next Steps:")
    print("1. Open the Streamlit app at http://localhost:8501")
    print("2. Navigate to 'üéØ Stage 2: Label Quotes' section")
    print("3. You should see the labeled quotes with color-coded sentiment")
    print("4. Check the sentiment legend and relevance score interpretation")

if __name__ == "__main__":
    test_ui_scoring_display() 