Buried Wins Finding Production Standard: Criteria and Articulation FrameworkDefinition:A Finding is a discrete, noteworthy observation extracted from an individual response record, which reveals a material issue, opportunity, or actionable insight for a B2B SaaS client. It stands out from general sentiment or codes because it is likely to influence executive decision-making or product/market strategy. Every finding must be articulated with sufficient specificity to stand alone as an actionable business insight, derived exclusively from response data without solutioning, and structured to enable clustering with similar findings to construct robust themes.Linguistic Standards: Executive Brevity with Technical PrecisionTone Requirements:? Confident and Evidence-Based: Use "reveals," "demonstrates," "shows" - avoid tentative language like "suggests" or "indicates"? Direct without Editorializing: State what happened, not what it means emotionally? Urgency when Warranted: Critical findings (9+ points) require immediate attention language? Business-Focused: Every statement connects to revenue, competition, or operational impactStylistic Mandates:? Sentence Length: Maximum 25 words for impact statements, 35 words for evidence/context? Active Voice Only: "Churn increased 10%" not "10% churn was experienced"? Lead with Numbers: When quantifiable data exists, start with the metric? Executive Scan-ability: Key insights digestible in 10-second read? No Research Jargon: Eliminate "participants," "data suggests," "analysis shows," "findings indicate"Specificity Requirements:? Name Everything: Always use specific integration names, competitor names, feature names? Quantify When Possible: Include timeframes, percentages, dollar amounts from response data? Avoid Generic Terms: Never use "customers," "users," "stakeholders" without qualification? Technical Precision: Use exact terminology mentioned in response (API, CRM, HRIS, etc.)AI Training Consistency:? Template Adherence: Every finding follows identical IMPACT ? EVIDENCE ? CONTEXT structure? Standardized Terminology: Use same business terms consistently throughout (don't alternate synonyms)? Pattern Recognition: Similar operational details use identical phrasing for theme clustering? Quality Gates: Each element has measurable criteria (word count, structure, content requirements)Executive Communication Standards:? Business Language: Revenue impact, competitive positioning, strategic implications, operational efficiency? C-Suite Relevance: Every finding answers "why should the CEO care about this?"? Action Orientation: Focus on what needs attention, not what went well (unless novelty warrants)? Competitive Intelligence: Frame insights in market context, not internal process contextWeighted Scoring Criteria:Findings are evaluated using a weighted point system. Each criterion below carries the specified point value.CriterionPointsDescriptionQuantitative ThresholdsSample Trigger Questions for Analyst/AINovelty3The observation is new/unexpected for the client, challenging assumptions or established beliefs about their market/product. Reveals insights executives haven't previously recognized.Contradicts documented client assumptions, previous research, or established product beliefs. Provides "aha moment" potential.Is this something the client has not previously recognized or challenges their current understanding? Would this surprise the executive team?Tension/Contrast3The finding exposes deep, specific tension around competitors, business conditions, or significant tradeoffs (e.g., "We chose Competitor X over you because of Y, even though we preferred your UI"). Reveals specific competitive dynamics or business friction.Identifies explicit conflict involving: named competitors, specific business constraints, regulatory/compliance issues, or strategic contradictionsDoes this highlight specific competitive pressure, named competitor advantage, regulatory blocker, or strategic business dilemma?Materiality2The finding has meaningful business impactÑaffecting revenue, customer satisfaction, retention, or competitive positioning. Includes high-influence sources: C-suite, VP, Director, Lead Buyer, Economic Buyer, or stakeholders controlling >$50k budget decisions.¥ Revenue impact: >$10k ARR affected<br>¥ Retention: >5% change in cohort behavior<br>¥ Competitive: Affects win/loss rate<br>¥ High-influence roles: C-suite, VP, Director, Lead Buyer, Economic Buyer, budget authority >$50kDoes this affect a key business KPI, major customer segment, or come from a decision-making stakeholder?Actionability2The observation suggests a clear step, fix, or action the client could take to improve outcomes (e.g., change a process, add a feature, address a risk).Must identify specific action within client's control (product change, process modification, go-to-market adjustment)Could this directly inform a roadmap item, process change, or go-to-market plan within 6 months?Specificity2The finding is precise, detailed, and not generic. It references a particular feature, workflow, market condition, or user group.Must reference specific: product feature, workflow step, integration, user role, or measurable processIs this about a concrete product aspect, named integration, specific workflow, or defined user segment?Metric/Quantification1The finding is supported by a tangible metric, timeframe, or quantifiable outcome (e.g., "Churn increased by 15% after X change").Includes specific: percentage, dollar amount, timeframe, headcount, or measurable business metricIs there a number, timeframe, or business measure that quantifies the impact?Scoring Thresholds:? 5+ points = Finding (include in analysis)? 7+ points = Priority Finding (highlight in executive summary)? 9+ points = Critical Finding (immediate client attention required)Operational Definitions for Scoring Consistency:"Meaningful Business Impact" Criteria (Materiality):Award 2 points if response contains ANY of:? Named dollar amount (any size)? Percentage change >1%? Timeline impact >1 week? Affects >10% of user base/customers? Blocks/enables deal closure? Compliance/regulatory requirements mentioned"High-Influence Source" Criteria (Materiality):Award 2 points if response mentions:? Specific titles: C-suite, VP, Director, Lead Buyer, Economic Buyer? Budget authority statements ("I control X budget," "I approve purchases")? Decision-making language ("I make the final decision," "my approval required")"C-Suite Relevance" Test:Finding qualifies if it would affect:? Revenue (any measurable amount)? Competitive win/loss rate? Customer retention >5%? Implementation timeline >2 weeks? Compliance/regulatory requirements? Named competitor comparisonEdge Case Decision Rules:Borderline Scoring (4 points):IF score = 4 points:- AND contains named competitor ? Finding- AND contains specific metric/timeframe ? Finding  - ELSE ? Not a findingMultiple Issues in Single Response:? Create separate findings for each distinct operational detail? DO NOT combine unless same root cause/processUnclear Participant Context:? Score based on response content only? Do not assume Materiality points for unstated roles? Focus on operational details and business impact mentionedAmbiguous Response Data:WHEN response data is vague:- Use exact language from response- Do not interpret or enhance meaning- Focus on concrete details providedWHEN conflicting information in single response:- Choose most specific/quantified element- Note apparent contradiction in evidence sectionConflicting Criteria Scenarios:WHEN criteria conflict:- High Novelty (3pts) + Low Materiality (0pts) = Focus on surprise/assumption-breaking element- High Materiality (2pts) + Low Specificity (0pts) = Add specific details from response- Multiple criteria borderline = Default to LOWER score unless clear evidenceIF uncertain about any criterion:- Default to NOT awarding points- Explain reasoning in score justification- Flag for human review if neededRequired Structure: IMPACT ? EVIDENCE ? CONTEXT1. IMPACT STATEMENT (Lead Sentence)Opens with the business consequence revealed in the responseFormat: "[Business impact/risk/opportunity] [affecting scope] [due to specific condition]"Examples:? ? Strong: "Enterprise deal velocity slows significantly when Salesforce integration requirements emerge mid-cycle"? ? Strong: "Technical buyers override user preference decisions when integration complexity exceeds internal capabilities"? ? Strong: "Onboarding timeline expectations create deal-breaking friction for time-sensitive implementations"? ? Weak: "Integration challenges impact customer success"Flexibility Guidelines:? When financial data available: Include specific amounts/percentages? When financial data unavailable: Focus on operational impact (timeline, process, decision)? When scope unclear: Use "key stakeholders" or "decision-making process"2. EVIDENCE SPECIFICATION (Supporting Detail)Synthesizes the specific observation from response data into a clear, theme-ready statementFormat: "[Specific operational detail] [with quantifiable context] [under defined business conditions]"Examples:? ? Strong: "Salesforce integration required three months of custom development and two complete workflow rebuilds, nearly causing project abandonment"? ? Strong: "Technical decision overruled user preference, selecting Competitor X for superior Kubernetes integration despite inferior UI experience"? ? Strong: "API documentation inadequacy necessitated constant support tickets, extending implementation timeline from two weeks to three months"? ? Strong: "SOC 2 compliance absence eliminated vendor consideration for 60% of enterprise buyer evaluations"? ? Weak: "Integration difficulties were experienced"? ? Weak: "Customers mentioned competitor advantages"Quality Checks:? Synthesizes response content into specific operational statements? Includes quantifiable details when present (timeframes, percentages, scope)? Structures information for easy theme clustering with similar findings? Avoids verbatim quotes unless they meet finding-level standards3. STRATEGIC CONTEXT (Business Significance)One to two complete sentences explaining what this reveals about market dynamics, competitive positioning, or buyer behaviorFormat: "[What the pattern reveals about buyer/market behavior]. [Competitive/business implication if applicable]."Examples:? ? Strong: "Technical requirements systematically trump user experience preferences in enterprise software decisions when internal development resources are constrained."? ? Strong: "Implementation timeline expectations have become a primary competitive differentiator, with buyers willing to sacrifice functionality for rapid deployment."? ? Strong: "Marketing positioning often misaligns with technical reality when assumed 'out-of-box' functionality requires significant custom development. Enterprise buyers increasingly prioritize pre-built integrations over feature richness."? ? Weak: "This affects competitive positioning"Quality Checks:? Self-contained sentences that don't reference previous content? Maximum two sentences? References patterns or trends implied by the response? Connects to broader business dynamics without requiring external dataCriterion-Specific Language Requirements:High-Novelty Findings (3 pts)? Lead with: "Contrary to expectations..." or "Unexpectedly..." or "Response reveals..."? Emphasize: What assumption or belief this challenges? Focus: On the surprising or assumption-breaking nature of the observationHigh-Tension Findings (3 pts)? Structure: "[Desired outcome] conflicted with [blocking constraint]"? Highlight: Specific tradeoff or dilemma revealed in response? Include: What was at stake in the decision? Name: Competitors when explicitly mentioned in responseHigh-Materiality Findings (2 pts)? Quantify: When financial/operational metrics are provided in response? Estimate: Business impact when specific numbers unavailable ("significant revenue impact," "multiple enterprise deals")? Connect: To business consequences mentioned in responseHigh-Actionability Findings (2 pts)? Focus: On what the response reveals needs to be addressed? Identify: Clear gaps or opportunities mentioned by respondent? Avoid: Prescribing solutions (describe the revealed need, not the fix)High-Specificity Findings (2 pts)? Name: Specific features, integrations, or workflows mentioned in response? Avoid: Generic terms unless that's how respondent described it? Include: Technical or process details provided by respondentQuantified Findings (1 pt)? Lead: With numbers when provided in response? Contextualize: What respondent said the metric means? Compare: To benchmarks only if mentioned in responseIntegrated Examples (B2B SaaS Context):ExampleCriteria MetPointsComplete Finding ArticulationCritical FindingNovelty (3), Tension/Contrast (3), Specificity (2), Materiality (2)10Impact: Technical buyers override user preference decisions when integration complexity exceeds internal capabilities<br>Evidence: Technical decision overruled user preference, selecting Competitor X for superior Kubernetes integration despite team preferring client UI and superior user experience<br>Context: Enterprise software decisions systematically prioritize technical requirements over user experience when internal development resources are constrained.Priority FindingMateriality (2), Actionability (2), Specificity (2), Metric/Quantification (1)7Impact: Enterprise deal velocity slows significantly when API documentation inadequacy extends implementation timelines<br>Evidence: API documentation inadequacy necessitated constant support tickets, extending implementation timeline from two weeks to three months<br>Context: Implementation timeline expectations have become a primary competitive differentiator, with buyers willing to sacrifice functionality for rapid deployment.Priority FindingMateriality (2), Actionability (2), Specificity (2), Metric/Quantification (1)7Impact: Revenue impact accelerates when UI design changes conceal essential functionality from existing users<br>Evidence: UI redesign concealed frequently-used features, resulting in 10% quarter-over-quarter churn increase and user complaints about workflow disruption<br>Context: User interface changes that prioritize aesthetics over functional accessibility create immediate retention risks for established customer bases.FindingNovelty (3), Metric/Quantification (1), Specificity (2)6Impact: Onboarding timeline expectations create competitive differentiation opportunities for rapid implementation<br>Evidence: Team achieved full system deployment within two days, exceeding all previous vendor implementation experiences<br>Context: Rapid onboarding capabilities have become unexpected competitive advantages, challenging assumptions about complex software requiring extended implementation periods.FindingMateriality (2), Metric/Quantification (1), Specificity (2)5Impact: Compliance requirements eliminate vendor consideration during enterprise evaluation processes<br>Evidence: SOC 2 compliance absence eliminated vendor consideration for 60% of enterprise buyer evaluations<br>Context: Security compliance has become a binary qualifier rather than differentiator in enterprise software selection processes.Instructions for AI/Analyst Implementation:Step 1: Score Each Response Record? Review each response record for content meeting criteria? Assign points for each criterion met (using the point values above)? Apply operational definitions and edge case rules? Calculate total scoreStep 2: Classify and Articulate Findings? 9+ points: Critical Finding - Full articulation required? 7-8 points: Priority Finding - Full articulation required? 5-6 points: Standard Finding - Full articulation required? <5 points: Not a finding - exclude from analysisStep 3: Document Complete FindingFor each finding, provide:? Score Justification: "[X points: Criteria breakdown]"? Complete Articulation: Impact + Evidence + Context using framework structure? Source Attribution: Response record ID, deal context if availableStep 4: Self-Validation Before SubmissionREQUIRED: Complete this checklist for every finding:? [ ] Score Accuracy: Re-calculate points and confirm threshold met? [ ] Word Count: Impact ²25 words, Evidence/Context ²35 words each? [ ] Structure Verification: Impact sentence + Evidence sentence + Context sentence(s)? [ ] Source Traceability: Every claim traceable to response record content? [ ] Anti-Pattern Scan: No "should," "recommend," "suggest," "this reveals," research jargon? [ ] Specificity Check: Names, numbers, timeframes included where availableStep 5: Quality Recovery (if needed)IF finding seems generic after writing:? Check response for specific names, numbers, timeframes? Add operational details from response data? Remove abstract language ("challenges," "issues," "problems")? Rewrite with concrete terminology from responseIF score doesn't match articulation quality:? Verify point calculation accuracy using operational definitions? Ensure evidence supports claimed business impact? Confirm criterion-specific language requirements met? Check that all claims derive from response contentStep 6: Prepare for Theme Development? Tag findings with operational specifics for clustering? Prioritize higher-scoring findings for theme leadership? Ensure findings are structured for pattern recognition across similar business contextsQuality Assurance Checklist:Response Data Fidelity:? [ ] Is every claim traceable to specific response content?? [ ] Is evidence synthesis faithful to original meaning?? [ ] Is context derived from response data, not external assumptions?? [ ] Are quotes accurate or paraphrasing precise?Scoring Accuracy:? [ ] Has operational definition criteria been applied correctly?? [ ] Are edge case decision rules followed for borderline scores?? [ ] Is point calculation verified and threshold confirmed?? [ ] Is score justification clear and accurate?Specificity Standards:? [ ] Could this finding apply to any B2B SaaS company? (If yes, add response-specific details)? [ ] Are all terms and processes named as specifically as mentioned in response?? [ ] Is the business context as specific as the response data allows?? [ ] Are concrete details prioritized over abstract concepts?Business Impact Verification:? [ ] Is the business consequence clearly articulated using operational definitions?? [ ] Would the impact be meaningful to executive stakeholders?? [ ] Does this reveal something significant about market/buyer behavior?? [ ] Is C-suite relevance test satisfied?Articulation Quality:? [ ] Does the finding follow IMPACT ? EVIDENCE ? CONTEXT structure?? [ ] Are word count limits respected (25/35 words)?? [ ] Are criterion-specific language requirements met?? [ ] Is the finding ready for theme clustering with similar findings?? [ ] Is active voice used throughout?Error Prevention:? [ ] Have all anti-patterns been avoided?? [ ] Is research jargon eliminated?? [ ] Are solutioning statements removed?? [ ] Is referential language avoided?? [ ] Has the self-validation checklist been completed?Templates for Implementation:Complete Finding Template:Finding Title: [Specific business impact in 6-8 words]Score: [X points: Criteria breakdown]Impact: [Business consequence/risk/opportunity] affecting [scope] due to [specific condition]Evidence: [Specific operational detail] [with quantifiable context] [under defined business conditions]Context: [What pattern reveals about buyer/market behavior]. [Competitive/business implication if applicable].Source: [Response record ID, deal context if available]Flexible Formats for Different Scenarios:When Financial Data Available: "[Specific business impact with numbers] due to [specific condition] affecting [defined scope]"When Financial Data Unavailable: "[Operational/strategic impact] creates [business risk/opportunity] for [scope/process]"When Competitor Named: "[Impact] as [competitor] provides [specific advantage mentioned]"When Competitor Not Named: "[Impact] due to competitive alternatives offering [advantage mentioned]"Anti-Patterns to Avoid:? Solutioning:? "Company should implement..."? "Recommendation is to..."? "Next steps include..."? Generic Language:? "Customers want better experience" (unless that's the exact quote)? "Integration issues occurred" (be specific about what happened)? "Problems were identified" (describe the actual issue)? Research Jargon:? "Data suggests..."? "Findings indicate..."? "Analysis shows..."? Referential Language:? "This reveals..."? "This demonstrates..."? "This indicates..."? External Assumptions:? Adding context not in response data? Assuming competitive landscape knowledge? Inferring financial impact not mentionedCross-SaaS Application Examples:HR/People Management SaaS: "Payroll processing delays create compliance risks when HRIS integration complexity exceeds IT team capabilities during quarterly reporting periods"Marketing Automation SaaS: "Campaign deployment timelines extend significantly when CRM data synchronization requires manual intervention every workflow execution"Cybersecurity SaaS: "Security policy implementation decisions prioritize compliance requirements over user workflow efficiency when audit deadlines create time pressure"Developer Tools SaaS: "Technical integration complexity overrides cost considerations when API documentation quality affects development team productivity"1