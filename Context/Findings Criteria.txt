<buried_wins_findings_criteria>  <definition>    <description>A Finding is a discrete, noteworthy observation or customer quote extracted from the response data table, which reveals a material issue, opportunity, or actionable insight for a B2B SaaS client. It stands out from general sentiment or codes because it is likely to influence executive decision-making or product/market strategy.</description>    <minimum_requirement>A Finding should meet at least two of the following criteria</minimum_requirement>    <priority_note>The more criteria a record meets, the higher its priority as a finding</priority_note>  </definition>  <evaluation_criteria>    <criterion id="novelty">      <name>Novelty</name>      <description>The observation is new/unexpected for the client, challenging assumptions or established beliefs.</description>      <trigger_question>Is this something the client has not previously recognized?</trigger_question>    </criterion>    <criterion id="actionability">      <name>Actionability</name>      <description>The observation suggests a clear step, fix, or action the client could take to improve outcomes (e.g., change a process, add a feature, address a risk).</description>      <trigger_question>Could this directly inform a roadmap item or go-to-market plan?</trigger_question>    </criterion>    <criterion id="specificity">      <name>Specificity</name>      <description>The finding is precise, detailed, and not generic. It references a particular feature, workflow, market condition, or user group.</description>      <trigger_question>Is this about a concrete product aspect, metric, or process?</trigger_question>    </criterion>    <criterion id="materiality">      <name>Materiality</name>      <description>The finding has meaningful business impact—affecting revenue, customer satisfaction, retention, or competitive positioning.</description>      <trigger_question>Does this affect a key business KPI or customer segment?</trigger_question>    </criterion>    <criterion id="recurrence">      <name>Recurrence</name>      <description>The same observation (or near-identical quote) appears across multiple interviews or sources, suggesting a theme.</description>      <trigger_question>Is this echoed by two or more stakeholders/roles?</trigger_question>    </criterion>    <criterion id="stakeholder_weight">      <name>Stakeholder Weight</name>      <description>The observation comes from a high-influence decision maker (e.g., C-suite, VP, lead buyer) or a critical user persona.</description>      <trigger_question>Does the source matter to the client's business priorities?</trigger_question>    </criterion>    <criterion id="tension_contrast">      <name>Tension/Contrast</name>      <description>The finding exposes a tension, tradeoff, or significant contrast (e.g., "We love X, but can't use it because of Y"), revealing friction or opportunity.</description>      <trigger_question>Does this highlight a dilemma, blocker, or gap vs. competitors?</trigger_question>    </criterion>    <criterion id="metric_quantification">      <name>Metric/Quantification</name>      <description>The finding is supported by a tangible metric, timeframe, or quantifiable outcome (e.g., "Churn increased by 15% after X change").</description>      <trigger_question>Is there a number or business measure that makes this impactful?</trigger_question>    </criterion>  </evaluation_criteria>  <practical_examples>    <context>B2B SaaS Context</context>        <example id="1">      <quote>Integration with Salesforce required a $20k custom build. We nearly dropped the project.</quote>      <criteria_met>Specificity, Materiality, Actionability</criteria_met>    </example>    <example id="2">      <quote>Three out of five buyers cited lack of SOC 2 compliance as a dealbreaker.</quote>      <criteria_met>Recurrence, Materiality, Stakeholder Weight</criteria_met>    </example>    <example id="3">      <quote>The onboarding process is the best we've seen—our team was live in two days.</quote>      <criteria_met>Novelty, Metric/Quantification</criteria_met>    </example>    <example id="4">      <quote>We're actively considering switching due to inconsistent API documentation, which slows our development sprints by a week every release.</quote>      <criteria_met>Actionability, Specificity, Tension</criteria_met>    </example>    <example id="5">      <quote>Churn rose 10% quarter-over-quarter after the UI redesign; users complained about hidden features.</quote>      <criteria_met>Metric/Quantification, Materiality</criteria_met>    </example>    <example id="6">      <quote>Product Marketers need the ability to self-serve reports—waiting on data teams costs us hours every week.</quote>      <criteria_met>Stakeholder Weight, Actionability</criteria_met>    </example>  </practical_examples>  <analysis_instructions>    <step number="1">      <action>Review each coded response record</action>    </step>    <step number="2">      <action>For each record, check how many criteria are met (from the list above)</action>      <conditions>        <condition>If a record meets at least two criteria, flag as a "finding"</condition>        <condition>If it meets three or more, mark as "priority finding"</condition>      </conditions>    </step>    <step number="3">      <action>For flagged findings, add brief justification</action>      <example>Referenced by 4 customers, affects revenue, specific to API performance</example>    </step>    <step number="4">      <action>Carry forward findings to the next step—pattern recognition and theme development</action>    </step>  </analysis_instructions>  <training_consistency>    <optional_tools>      <tool>Provide analysts or prompt engineers with a "finding checklist"</tool>      <tool>Embed the criteria as system messages for your LLM</tool>    </optional_tools>  </training_consistency>  <reporting_integration>    <finding_eligibility>      <rule>Only findings (not all codes) are eligible for inclusion in the main deck of reports</rule>    </finding_eligibility>    <traceability_requirements>      <requirement>All findings must be traceable to source data</requirement>      <requirement>When possible, attributed to a specific participant role and deal status</requirement>    </traceability_requirements>    <theme_development>      <definition>Themes in reports are collections of validated, recurring findings</definition>      <labeling_categories>        <category>Barrier</category>        <category>Opportunity</category>        <category>Strategic</category>        <category>Functional</category>      </labeling_categories>    </theme_development>  </reporting_integration></buried_wins_findings_criteria>