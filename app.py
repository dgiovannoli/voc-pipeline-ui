import os
import sys
import subprocess
from datetime import date, datetime
import pathlib
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
import re
from supabase_database import SupabaseDatabase
from prompts.core_extraction import CORE_EXTRACTION_PROMPT
import yaml
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json

# Initialize Supabase database
try:
    db = SupabaseDatabase()
    SUPABASE_AVAILABLE = True
except Exception as e:
    SUPABASE_AVAILABLE = False
    st.error(f"‚ùå Failed to connect to Supabase: {e}")

# Debug information (commented out for production)
# st.write("Current working directory:", os.getcwd())

# Helper functions
def extract_interviewee_and_company(filename):
    """
    Extract interviewee and company from filename.
    Example: 'Interview with Ben Evenstad, Owner at Evenstad Law.docx' -> ('Ben Evenstad', 'Evenstad Law')
    """
    base = os.path.basename(filename).replace('.docx', '').replace('.txt', '')
    
    # Handle simple filenames without interview format
    if not base.lower().startswith("interview with "):
        return ("Unknown", "Unknown")
    
    base = base[len("interview with "):]
    parts = [p.strip() for p in base.split(",")]
    interviewee = parts[0] if parts else "Unknown"
    company = "Unknown"
    for part in parts[1:]:
        match = re.search(r'at (.+)', part, re.IGNORECASE)
        if match:
            company = match.group(1).strip()
            break
    return interviewee, company

def save_uploaded_files(uploaded_files, upload_dir="uploads"):
    """
    Save uploaded Streamlit files to disk and return a list of file paths.
    """
    if not os.path.exists(upload_dir):
        os.makedirs(upload_dir)
    saved_paths = []
    for uploaded_file in uploaded_files:
        file_path = os.path.join(upload_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        saved_paths.append(file_path)
    return saved_paths

@st.cache_data(ttl=1, show_spinner=False)
def load_csv(path):
    try:
        return pd.read_csv(path)
    except Exception as e:
        st.error(f"Error loading CSV from {path}: {e}")
        return pd.DataFrame()

def process_files():
    """Process uploaded files using the modular pipeline CLI"""
    if not st.session_state.uploaded_paths:
        raise Exception("No files uploaded")
    
    all_results = []
    processed_count = 0
    
    for i, path in enumerate(st.session_state.uploaded_paths):
        interviewee, company = extract_interviewee_and_company(os.path.basename(path))
        
        # Create temporary output file for this file
        temp_output = BASE / f"temp_output_{i}.csv"
        
        result = subprocess.run([
            sys.executable, "-m", "voc_pipeline.modular_cli", "extract-core",
            path,
            company if company else "Unknown",
            interviewee if interviewee else "Unknown",
            "closed_won",
            "2024-01-01",
            "-o", str(temp_output)
        ], capture_output=True, text=True)
        
        if result.returncode != 0:
            st.error(f"Processing failed for {os.path.basename(path)}!\n\nSTDOUT:\n{result.stdout}\n\nSTDERR:\n{result.stderr}")
            # Clean up temp files
            for temp_file in [BASE / f"temp_output_{j}.csv" for j in range(len(st.session_state.uploaded_paths))]:
                if temp_file.exists():
                    temp_file.unlink()
            return
        
        # Load and combine results
        if temp_output.exists() and temp_output.stat().st_size > 0:
            try:
                df_temp = pd.read_csv(temp_output)
                all_results.append(df_temp)
                processed_count += 1
                st.success(f"‚úÖ Processed {os.path.basename(path)}: {len(df_temp)} responses")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Could not read results from {os.path.basename(path)}: {e}")
        
        # Clean up temp file
        if temp_output.exists():
            temp_output.unlink()
    
    # Combine all results
    if all_results:
        combined_df = pd.concat(all_results, ignore_index=True)
        combined_df.to_csv(STAGE1_CSV, index=False)
        st.success(f"üéâ Successfully processed {processed_count} files with {len(combined_df)} total responses")
    else:
        st.error("Processing failed: No output was generated from any files. Please check your input files and try again.")
        return
    
    st.session_state.current_step = 2

def validate_and_build():
    """Run validation and build final table"""
    if os.path.exists(STAGE1_CSV):
        # Validate
        subprocess.run([
            sys.executable, "-m", "voc_pipeline", "validate",
            "--input", str(STAGE1_CSV),
            "--output", str(VALIDATED_CSV)
        ], check=True)
        
        # Build table
        subprocess.run([
            sys.executable, "-m", "voc_pipeline", "build-table",
            "--input", str(VALIDATED_CSV),
            "--output", str(RESPONSE_TABLE_CSV)
        ], check=True)

def load_prompt_template():
    prompt_path = os.path.join("prompts", "core_extraction.py")
    try:
        with open(prompt_path, "r") as f:
            return f.read()
    except Exception as e:
        return f"Could not load prompt template: {e}"

PROCESSING_DETAILS = """
### Processing Details

How the pipeline processes your interviews (16K-optimized version):

**Batching & Parallel Processing:**
- Multiple interviews are processed in parallel using Python's `ThreadPoolExecutor` for speed and efficiency.
- Each file is handled as a separate job, so you can upload and process many interviews at once.

**Advanced Chunking & Segmentation (16K Optimized):**
- Uses token-based chunking with ~2K tokens per chunk (vs previous 2K character chunks)
- Q&A-aware segmentation that preserves conversation boundaries and context
- Intelligent overlap of 200 tokens to maintain continuity between chunks
- Preserves full Q&A exchanges and speaker turns within chunks
- Adaptive chunking that respects token limits while maximizing context

**Enhanced LLM Processing:**
- Each chunk is sent to GPT-3.5-turbo-16k with comprehensive context
- Extracts 3-5 insights per chunk (vs previous 1 insight per chunk)
- Preserves much longer verbatim responses (200-800 words vs previous 20-50 words)
- Focuses on detailed customer experiences, quantitative feedback, and specific examples

**Improved Content Extraction:**
- Prioritizes detailed customer experiences with specific scenarios and examples
- Captures quantitative feedback (metrics, timelines, ROI discussions)
- Preserves comparative analysis and competitive evaluations
- Maintains integration requirements and workflow details

**Quality Assurance:**
- Enhanced validation for richer, more contextually complete responses
- Quality logging tracks response length and content preservation
- Flags responses that lose too much context during processing

**Performance:**
- Fewer API calls due to larger chunks (more efficient token usage)
- Higher quality insights due to better context preservation
- Richer verbatim responses with full context and examples

---
This pipeline is optimized for the 16K token context window to deliver significantly richer insights and more complete verbatim responses.
"""

# Constants
BASE = pathlib.Path(__file__).parent
STAGE1_CSV = BASE / "stage1_output.csv"
VALIDATED_CSV = BASE / "validated_quotes.csv"
RESPONSE_TABLE_CSV = BASE / "response_data_table.csv"

def process_files_with_progress():
    """Process uploaded files with progress tracking"""
    if not st.session_state.uploaded_paths:
        st.error("No files uploaded")
        return
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_files = len(st.session_state.uploaded_paths)
    all_results = []
    
    for i, path in enumerate(st.session_state.uploaded_paths):
        status_text.text(f"Processing {os.path.basename(path)}...")
        
        interviewee, company = extract_interviewee_and_company(os.path.basename(path))
        temp_output = BASE / f"temp_output_{i}.csv"
        
        result = subprocess.run([
            sys.executable, "-m", "voc_pipeline.modular_cli", "extract-core",
            path, company, interviewee, "closed_won", "2024-01-01", "-o", str(temp_output)
        ], capture_output=True, text=True)
        
        if result.returncode == 0 and temp_output.exists():
            try:
                df_temp = pd.read_csv(temp_output)
                all_results.append(df_temp)
                st.success(f"‚úÖ {os.path.basename(path)}: {len(df_temp)} responses")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è {os.path.basename(path)}: {e}")
        
        if temp_output.exists():
            temp_output.unlink()
        
        progress_bar.progress((i + 1) / total_files)
    
    if all_results:
        combined_df = pd.concat(all_results, ignore_index=True)
        combined_df.to_csv(STAGE1_CSV, index=False)
        st.success(f"üéâ Processed {len(all_results)} files with {len(combined_df)} total responses")
        st.session_state.current_step = 2
    else:
        st.error("No results generated")

def save_stage1_to_supabase(csv_path):
    """Save Stage 1 results to Supabase"""
    if not SUPABASE_AVAILABLE:
        st.error("‚ùå Supabase not available")
        return False
    
    try:
        df = pd.read_csv(csv_path)
        saved_count = 0
        
        for _, row in df.iterrows():
            response_data = {
                'response_id': row.get('Response ID', ''),
                'verbatim_response': row.get('Verbatim Response', ''),
                'subject': row.get('Subject', ''),
                'question': row.get('Question', ''),
                'deal_status': row.get('Deal Status', 'closed_won'),
                'company': row.get('Company Name', ''),
                'interviewee_name': row.get('Interviewee Name', ''),
                'interview_date': row.get('Date of Interview', '2024-01-01'),
                'file_source': 'stage1_processing'
            }
            
            if db.save_core_response(response_data):
                saved_count += 1
        
        st.success(f"‚úÖ Saved {saved_count} responses to Supabase")
        return True
        
    except Exception as e:
        st.error(f"‚ùå Failed to save to Supabase: {e}")
        return False

def run_stage2_analysis():
    """Run Stage 2 analysis using Supabase"""
    if not SUPABASE_AVAILABLE:
        st.error("‚ùå Supabase not available")
        return None
    
    try:
        from enhanced_stage2_analyzer import run_supabase_analysis
        result = run_supabase_analysis()
        return result
    except Exception as e:
        st.error(f"‚ùå Stage 2 analysis failed: {e}")
        return None

def get_stage2_summary():
    """Get Stage 2 summary from Supabase"""
    if not SUPABASE_AVAILABLE:
        return None
    
    try:
        summary = db.get_summary_statistics()
        return summary
    except Exception as e:
        st.error(f"‚ùå Failed to get summary: {e}")
        return None

def show_supabase_status():
    """Show Supabase connection status and data summary"""
    st.subheader("üóÑÔ∏è Supabase Database Status")
    
    if not SUPABASE_AVAILABLE:
        st.error("‚ùå Supabase not connected")
        st.info("üí° Make sure your .env file contains SUPABASE_URL and SUPABASE_ANON_KEY")
        return
    
    try:
        # Get summary statistics
        summary = db.get_summary_statistics()
        
        if "error" in summary:
            st.error(f"‚ùå Database error: {summary['error']}")
            return
        
        # Display statistics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Quotes", summary['total_quotes'])
        
        with col2:
            st.metric("Quotes Analyzed", summary['quotes_with_scores'])
        
        with col3:
            st.metric("Coverage", f"{summary['coverage_percentage']}%")
        
        # Deal outcome distribution
        if summary['deal_outcome_distribution']:
            st.subheader("üè¢ Deal Outcome Distribution")
            deal_df = pd.DataFrame(list(summary['deal_outcome_distribution'].items()), 
                                 columns=['Status', 'Count'])
            
            fig = px.pie(deal_df, values='Count', names='Status', 
                        title="Deal Status Distribution")
            st.plotly_chart(fig, use_container_width=True)
        
        # Company distribution
        if summary['company_distribution']:
            st.subheader("üè¢ Company Distribution")
            company_df = pd.DataFrame(list(summary['company_distribution'].items()), 
                                    columns=['Company', 'Count'])
            company_df = company_df.sort_values('Count', ascending=False).head(10)
            
            fig = px.bar(company_df, x='Company', y='Count', 
                        title="Top 10 Companies")
            st.plotly_chart(fig, use_container_width=True)
        
        # Criteria performance
        if summary['criteria_performance']:
            st.subheader("üìä Criteria Performance")
            criteria_data = []
            for criterion, perf in summary['criteria_performance'].items():
                criteria_data.append({
                    'Criterion': criterion,
                    'Average Score': perf['average_score'],
                    'Mentions': perf['mention_count'],
                    'Coverage %': perf['coverage_percentage']
                })
            
            criteria_df = pd.DataFrame(criteria_data)
            criteria_df = criteria_df.sort_values('Average Score', ascending=False)
            
            fig = px.bar(criteria_df, x='Criterion', y='Average Score',
                        title="Average Scores by Criterion",
                        hover_data=['Mentions', 'Coverage %'])
            st.plotly_chart(fig, use_container_width=True)
            
            # Show detailed table
            st.dataframe(criteria_df, use_container_width=True)
    
    except Exception as e:
        st.error(f"‚ùå Error getting database status: {e}")

def main():
    st.set_page_config(
        page_title="VOC Pipeline UI",
        page_icon="üé§",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Initialize session state
    if 'current_step' not in st.session_state:
        st.session_state.current_step = 1
    if 'uploaded_paths' not in st.session_state:
        st.session_state.uploaded_paths = []
    
    # Sidebar
    with st.sidebar:
        st.title("üé§ VOC Pipeline")
        st.markdown("Voice of Customer Analysis")
        
        # Navigation
        st.subheader("üìã Navigation")
        if st.button("üè† Home", use_container_width=True):
            st.session_state.current_step = 1
        if st.button("üì§ Upload Files", use_container_width=True):
            st.session_state.current_step = 2
        if st.button("üìä Stage 1 Results", use_container_width=True):
            st.session_state.current_step = 3
        if st.button("üîç Stage 2 Analysis", use_container_width=True):
            st.session_state.current_step = 4
        if st.button("üìà Export & Share", use_container_width=True):
            st.session_state.current_step = 5
        if st.button("üóÑÔ∏è Database Status", use_container_width=True):
            st.session_state.current_step = 6
        if st.button("üìù Prompts & Details", use_container_width=True):
            st.session_state.current_step = 7
        
        # Supabase status
        st.subheader("üóÑÔ∏è Database")
        if SUPABASE_AVAILABLE:
            st.success("‚úÖ Supabase Connected")
        else:
            st.error("‚ùå Supabase Not Available")
    
    # Main content area
    if st.session_state.current_step == 1:
        show_welcome_screen()
    elif st.session_state.current_step == 2:
        show_upload_section()
    elif st.session_state.current_step == 3:
        show_stage1_results()
    elif st.session_state.current_step == 4:
        show_stage2_analysis()
    elif st.session_state.current_step == 5:
        show_export_section()
    elif st.session_state.current_step == 6:
        show_supabase_status()
    elif st.session_state.current_step == 7:
        show_prompts_details()

def show_upload_section():
    st.title("üì§ Upload Interview Files")
    st.markdown("Upload your interview transcripts for processing")
    
    uploaded_files = st.file_uploader(
        "Choose interview files",
        type=['txt', 'docx'],
        accept_multiple_files=True,
        help="Upload interview transcripts in .txt or .docx format"
    )
    
    if uploaded_files:
        st.session_state.uploaded_paths = save_uploaded_files(uploaded_files)
        st.success(f"‚úÖ Uploaded {len(uploaded_files)} files")
        
        # Show file details
        st.subheader("üìÅ Uploaded Files")
        for i, path in enumerate(st.session_state.uploaded_paths):
            interviewee, company = extract_interviewee_and_company(os.path.basename(path))
            st.write(f"**{i+1}.** {os.path.basename(path)}")
            st.write(f"   - Interviewee: {interviewee}")
            st.write(f"   - Company: {company}")
        
        # Process button
        if st.button("üöÄ Process Files", type="primary"):
            with st.spinner("Processing files..."):
                process_files_with_progress()

def show_stage1_results():
    st.title("üìä Stage 1 Results")
    
    if not os.path.exists(STAGE1_CSV):
        st.warning("‚ö†Ô∏è No Stage 1 results found. Please process some files first.")
        return
    
    # Load and display results
    df = load_csv(STAGE1_CSV)
    
    if df.empty:
        st.error("‚ùå No data found in Stage 1 results")
        return
    
    # Summary statistics
    st.subheader("üìà Summary")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Total Responses", len(df))
    
    with col2:
        unique_companies = df['Company Name'].nunique()
        st.metric("Unique Companies", unique_companies)
    
    with col3:
        unique_interviewees = df['Interviewee Name'].nunique()
        st.metric("Unique Interviewees", unique_interviewees)
    
    # Save to Supabase
    st.subheader("üíæ Save to Database")
    if st.button("üíæ Save to Supabase", type="primary"):
        if save_stage1_to_supabase(STAGE1_CSV):
            st.success("‚úÖ Data saved to Supabase successfully!")
    
    # Display data
    st.subheader("üìã Response Data")
    st.dataframe(df, use_container_width=True)
    
    # Download option
    csv = df.to_csv(index=False)
    st.download_button(
        label="üì• Download CSV",
        data=csv,
        file_name="stage1_results.csv",
        mime="text/csv"
    )

def show_stage2_analysis():
    st.title("üîç Stage 2 Analysis")
    
    if not SUPABASE_AVAILABLE:
        st.error("‚ùå Supabase not available. Please configure your database connection.")
        return
    
    # Check if we have data to analyze
    summary = get_stage2_summary()
    if not summary or summary.get('total_quotes', 0) == 0:
        st.warning("‚ö†Ô∏è No data found in database. Please upload and process files first.")
        return
    
    st.subheader("üìä Current Status")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Total Quotes", summary['total_quotes'])
    
    with col2:
        st.metric("Analyzed", summary['quotes_with_scores'])
    
    with col3:
        st.metric("Coverage", f"{summary['coverage_percentage']}%")
    
    # Run analysis
    st.subheader("üöÄ Run Analysis")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üîÑ Run Incremental Analysis", type="primary"):
            with st.spinner("Running Stage 2 analysis..."):
                result = run_stage2_analysis()
                if result:
                    st.success("‚úÖ Analysis completed successfully!")
                    st.rerun()
    
    with col2:
        if st.button("üîÑ Force Reprocess All", type="secondary"):
            with st.spinner("Force reprocessing all quotes..."):
                from enhanced_stage2_analyzer import run_supabase_analysis
                result = run_supabase_analysis(force_reprocess=True)
                if result:
                    st.success("‚úÖ Force reprocess completed!")
                    st.rerun()
    
    # Display results
    if summary['criteria_performance']:
        st.subheader("üìà Analysis Results")
        
        # Criteria performance chart
        criteria_data = []
        for criterion, perf in summary['criteria_performance'].items():
            criteria_data.append({
                'Criterion': criterion,
                'Average Score': perf['average_score'],
                'Mentions': perf['mention_count'],
                'Coverage %': perf['coverage_percentage']
            })
        
        criteria_df = pd.DataFrame(criteria_data)
        criteria_df = criteria_df.sort_values('Average Score', ascending=False)
        
        # Color coding for scores
        def score_color(val):
            if val >= 3.5:
                return 'background-color: #d4edda'  # Green for high scores
            elif val >= 2.5:
                return 'background-color: #fff3cd'  # Yellow for medium scores
            else:
                return 'background-color: #f8d7da'  # Red for low scores
        
        st.dataframe(criteria_df.style.applymap(score_color, subset=['Average Score']), 
                    use_container_width=True)
        
        # Chart
        fig = px.bar(criteria_df, x='Criterion', y='Average Score',
                    title="Average Scores by Criterion",
                    hover_data=['Mentions', 'Coverage %'])
        st.plotly_chart(fig, use_container_width=True)

def show_export_section():
    st.title("üìà Export & Share")
    
    if not SUPABASE_AVAILABLE:
        st.error("‚ùå Supabase not available")
        return
    
    # Get data summary
    summary = get_stage2_summary()
    if not summary:
        st.warning("‚ö†Ô∏è No data available for export")
        return
    
    st.subheader("üìä Data Summary")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Total Quotes", summary['total_quotes'])
    
    with col2:
        st.metric("Analyzed Quotes", summary['quotes_with_scores'])
    
    with col3:
        st.metric("Coverage", f"{summary['coverage_percentage']}%")
    
    # Export options
    st.subheader("üì§ Export Options")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üì• Export Core Responses", type="primary"):
            try:
                core_df = db.get_core_responses()
                if not core_df.empty:
                    csv = core_df.to_csv(index=False)
                    st.download_button(
                        label="üì• Download Core Responses CSV",
                        data=csv,
                        file_name=f"core_responses_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                else:
                    st.warning("No core responses to export")
            except Exception as e:
                st.error(f"Export failed: {e}")
    
    with col2:
        if st.button("üì• Export Quote Analysis", type="primary"):
            try:
                analysis_df = db.get_quote_analysis()
                if not analysis_df.empty:
                    csv = analysis_df.to_csv(index=False)
                    st.download_button(
                        label="üì• Download Quote Analysis CSV",
                        data=csv,
                        file_name=f"quote_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                else:
                    st.warning("No quote analysis to export")
            except Exception as e:
                st.error(f"Export failed: {e}")
    
    # Data preview
    st.subheader("üëÄ Data Preview")
    
    tab1, tab2 = st.tabs(["Core Responses", "Quote Analysis"])
    
    with tab1:
        core_df = db.get_core_responses()
        if not core_df.empty:
            st.dataframe(core_df.head(10), use_container_width=True)
        else:
            st.info("No core responses found")
    
    with tab2:
        analysis_df = db.get_quote_analysis()
        if not analysis_df.empty:
            st.dataframe(analysis_df.head(10), use_container_width=True)
        else:
            st.info("No quote analysis found")

def show_prompts_details():
    st.title("üìù Prompts & Processing Details")
    
    st.markdown(PROCESSING_DETAILS)
    
    st.subheader("üîß Configuration")
    
    # Show current configuration
    config_path = "config/analysis_config.yaml"
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config_content = f.read()
        st.code(config_content, language='yaml')
    else:
        st.info("No configuration file found")
    
    st.subheader("üìã Prompt Templates")
    
    # Show prompt template
    prompt_content = load_prompt_template()
    st.code(prompt_content, language='python')

def show_welcome_screen():
    st.title("üé§ Voice of Customer Pipeline")
    st.markdown("### AI-Powered Interview Analysis & Insights")
    
    st.markdown("""
    Welcome to the VOC Pipeline! This tool helps you extract valuable insights from customer interviews.
    
    **How it works:**
    1. **üì§ Upload** your interview transcripts (.txt or .docx files)
    2. **üîç Process** them through our AI pipeline to extract key insights
    3. **üìä Analyze** the responses against 10 executive criteria
    4. **üìà Export** and share your findings
    
    **Key Features:**
    - üöÄ **16K Token Optimization**: Processes longer, richer responses
    - üéØ **10-Criteria Analysis**: Comprehensive executive framework
    - üìä **Real-time Analytics**: Live dashboards and visualizations
    - ‚òÅÔ∏è **Cloud Storage**: All data stored securely in Supabase
    - üîÑ **Incremental Processing**: Only analyze new data
    """)
    
    # Quick start
    st.subheader("üöÄ Quick Start")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **1. Upload Files**
        - Go to the Upload Files section
        - Select your interview transcripts
        - Click "Process Files"
        """)
    
    with col2:
        st.markdown("""
        **2. View Results**
        - Check Stage 1 Results for extracted insights
        - Run Stage 2 Analysis for criteria scoring
        - Export your findings
        """)
    
    # Database status
    st.subheader("üóÑÔ∏è Database Status")
    if SUPABASE_AVAILABLE:
        st.success("‚úÖ Supabase Connected - Ready to process data")
        
        # Show quick stats
        try:
            summary = get_stage2_summary()
            if summary and 'total_quotes' in summary:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total Quotes", summary['total_quotes'])
                with col2:
                    st.metric("Analyzed", summary['quotes_with_scores'])
                with col3:
                    st.metric("Coverage", f"{summary['coverage_percentage']}%")
        except:
            st.info("No data in database yet")
    else:
        st.error("‚ùå Supabase Not Connected")
        st.info("Please configure your .env file with SUPABASE_URL and SUPABASE_ANON_KEY")

if __name__ == "__main__":
    main()


